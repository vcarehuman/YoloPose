{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-DeblurGANv2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1Cyaut6utnv",
        "colab_type": "text"
      },
      "source": [
        "# Colab-DeblurGANv2\n",
        "\n",
        "Currently only one picture as input allowed. \n",
        "\n",
        "Original repo: [TAMU-VITA/DeblurGANv2](https://github.com/TAMU-VITA/DeblurGANv2)\n",
        "\n",
        "My coalb fork: [styler00dollar/Colab-DeblurGANv2](https://github.com/styler00dollar/Colab-DeblurGANv2)\n",
        "\n",
        "Simple tutoiral:\n",
        "Place ```input.png``` in ```Google Drive/Colab-DeblurGANv2``` and run these cells. You can create this folder with colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUTg2s-by8si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check gpu\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mb3vhUOn5SzO",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Connect Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9OIL78n5p2E",
        "colab_type": "text"
      },
      "source": [
        "Either create input and output folders manually or with colab and place the files there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4FJ_R7d5W2a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title [Optional] Creating empty ```\"/content/drive/My Drive/Colab-DeblurGANv2/\"``` folder in Google Drive\n",
        "!mkdir \"/content/drive/My Drive/Colab-DeblurGANv2/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQbbcY7Ouzd_",
        "colab_type": "text"
      },
      "source": [
        "# Deblur with InceptionResNet-v2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G9sFBtrnmZz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Installing and downloading model\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-DeblurGANv2\n",
        "!pip install fire\n",
        "!pip install pretrainedmodels\n",
        "!pip install gdown\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!gdown --id 1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW6au4DWuJkV",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Changing predict.py\n",
        "%%writefile /content/Colab-DeblurGANv2/predict.py\n",
        "import os\n",
        "from glob import glob\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from fire import Fire\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aug import get_normalize\n",
        "from models.networks import get_generator\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, weights_path: str, model_name: str = ''):\n",
        "        with open('config/config.yaml') as cfg:\n",
        "            config = yaml.load(cfg)\n",
        "        model = get_generator(model_name or config['model'])\n",
        "        model.load_state_dict(torch.load(weights_path)['model'])\n",
        "        self.model = model.cuda()\n",
        "        self.model.train(True)\n",
        "        # GAN inference should be in train mode to use actual stats in norm layers,\n",
        "        # it's not a bug\n",
        "        self.normalize_fn = get_normalize()\n",
        "\n",
        "    @staticmethod\n",
        "    def _array_to_batch(x):\n",
        "        x = np.transpose(x, (2, 0, 1))\n",
        "        x = np.expand_dims(x, 0)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "    def _preprocess(self, x: np.ndarray, mask: Optional[np.ndarray]):\n",
        "        x, _ = self.normalize_fn(x, x)\n",
        "        if mask is None:\n",
        "            mask = np.ones_like(x, dtype=np.float32)\n",
        "        else:\n",
        "            mask = np.round(mask.astype('float32') / 255)\n",
        "\n",
        "        h, w, _ = x.shape\n",
        "        block_size = 32\n",
        "        min_height = (h // block_size + 1) * block_size\n",
        "        min_width = (w // block_size + 1) * block_size\n",
        "\n",
        "        pad_params = {'mode': 'constant',\n",
        "                      'constant_values': 0,\n",
        "                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n",
        "                      }\n",
        "        x = np.pad(x, **pad_params)\n",
        "        mask = np.pad(mask, **pad_params)\n",
        "\n",
        "        return map(self._array_to_batch, (x, mask)), h, w\n",
        "\n",
        "    @staticmethod\n",
        "    def _postprocess(x: torch.Tensor) -> np.ndarray:\n",
        "        x, = x\n",
        "        x = x.detach().cpu().float().numpy()\n",
        "        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "        return x.astype('uint8')\n",
        "\n",
        "    def __call__(self, img: np.ndarray, mask: Optional[np.ndarray], ignore_mask=True) -> np.ndarray:\n",
        "        (img, mask), h, w = self._preprocess(img, mask)\n",
        "        with torch.no_grad():\n",
        "            inputs = [img.cuda()]\n",
        "            if not ignore_mask:\n",
        "                inputs += [mask]\n",
        "            pred = self.model(*inputs)\n",
        "        return self._postprocess(pred)[:h, :w, :]\n",
        "\n",
        "def process_video(pairs, predictor, output_dir):\n",
        "    for video_filepath, mask in tqdm(pairs):\n",
        "        video_filename = os.path.basename(video_filepath)\n",
        "        output_filepath = os.path.join(output_dir, os.path.splitext(video_filename)[0]+'_deblur.mp4')\n",
        "        video_in = cv2.VideoCapture(video_filepath)\n",
        "        fps = video_in.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frame_num = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        video_out = cv2.VideoWriter(output_filepath, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
        "        tqdm.write(f'process {video_filepath} to {output_filepath}, {fps}fps, resolution: {width}x{height}')\n",
        "        for frame_num in tqdm(range(total_frame_num), desc=video_filename):\n",
        "            res, img = video_in.read()\n",
        "            if not res:\n",
        "                break\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred = predictor(img, mask)\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            video_out.write(pred)\n",
        "\n",
        "def main(img_pattern: str,\n",
        "         mask_pattern: Optional[str] = None,\n",
        "         weights_path='/content/Colab-DeblurGANv2/models/fpn_inception.h5',\n",
        "         out_dir='submit/',\n",
        "         side_by_side: bool = False,\n",
        "         video: bool = False):\n",
        "    def sorted_glob(pattern):\n",
        "        return sorted(glob(pattern))\n",
        "\n",
        "    imgs = sorted_glob(img_pattern)\n",
        "    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n",
        "    pairs = zip(imgs, masks)\n",
        "    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n",
        "    predictor = Predictor(weights_path=weights_path)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if not video:\n",
        "        for name, pair in tqdm(zip(names, pairs), total=len(names)):\n",
        "            f_img, f_mask = pair\n",
        "            img, mask = map(cv2.imread, (f_img, f_mask))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            pred = predictor(img, mask)\n",
        "            if side_by_side:\n",
        "                pred = np.hstack((img, pred))\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(os.path.join(out_dir, name),\n",
        "                        pred)\n",
        "    else:\n",
        "        process_video(pairs, predictor, out_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Fire(main)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lImGMorGKGTd",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Downloading model from my Google Drive (Very fast download)\n",
        "!mkdir /root/\n",
        "!mkdir /root/.cache/\n",
        "!mkdir /root/.cache/torch/\n",
        "!mkdir /root/.cache/torch/checkpoints/\n",
        "%cd /root/.cache/torch/checkpoints/\n",
        "!gdown --id 1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUQxom6W4YDw",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ```[OPTIONAL / ONLY RUN THIS IF YOU HAVE PROBLEMS WITH THE PREVIOUS CELL]``` Testrun to download model file with pretrainedmodels (Download takes around 10 minutes)\n",
        "%cd /content/\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/xinntao/ESRGAN/master/LR/baboon.png\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py /content/baboon.png\n",
        "%cd /content/\n",
        "!sudo rm /content/baboon.png\n",
        "!sudo rm /content/Colab-DeblurGANv2/submit/baboon.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9kImTgk3pncs",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run deblur and copy result go Google Drive\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py \"/content/drive/My Drive/Colab-DeblurGANv2/input.png\"\n",
        "!cp /content/Colab-DeblurGANv2/submit/input.png \"/content/drive/My Drive/Colab-DeblurGANv2/output.png\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Qucha-Su4J7",
        "colab_type": "text"
      },
      "source": [
        "# Deblur with MobileNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZmeIqZ8u8YL",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Installing and downloading models\n",
        "%cd /content/\n",
        "!git clone https://github.com/styler00dollar/Colab-DeblurGANv2\n",
        "!pip install fire\n",
        "!pip install pretrainedmodels\n",
        "!pip install gdown\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!gdown --id 1UXcsRVW-6KF23_TNzxw-xC0SzaMfXOaR\n",
        "!wget --no-check-certificate http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar\n",
        "!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-xu__FIp6Jz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Changing config.yaml\n",
        "%%writefile /content/Colab-DeblurGANv2/config/config.yaml\n",
        "---\n",
        "project: deblur_gan\n",
        "experiment_desc: fpn\n",
        "\n",
        "train:\n",
        "  files_a: &FILES_A /datasets/my_dataset/**/*.jpg\n",
        "  files_b: *FILES_A\n",
        "  size: &SIZE 256\n",
        "  crop: random\n",
        "  preload: &PRELOAD false\n",
        "  preload_size: &PRELOAD_SIZE 0\n",
        "  bounds: [0, .9]\n",
        "  scope: geometric\n",
        "  corrupt: &CORRUPT\n",
        "    - name: cutout\n",
        "      prob: 0.5\n",
        "      num_holes: 3\n",
        "      max_h_size: 25\n",
        "      max_w_size: 25\n",
        "    - name: jpeg\n",
        "      quality_lower: 70\n",
        "      quality_upper: 90\n",
        "    - name: motion_blur\n",
        "    - name: median_blur\n",
        "    - name: gamma\n",
        "    - name: rgb_shift\n",
        "    - name: hsv_shift\n",
        "    - name: sharpen\n",
        "\n",
        "val:\n",
        "  files_a: *FILES_A\n",
        "  files_b: *FILES_A\n",
        "  size: *SIZE\n",
        "  scope: geometric\n",
        "  crop: center\n",
        "  preload: *PRELOAD\n",
        "  preload_size: *PRELOAD_SIZE\n",
        "  bounds: [.9, 1]\n",
        "  corrupt: *CORRUPT\n",
        "\n",
        "phase: train\n",
        "warmup_num: 3\n",
        "model:\n",
        "  g_name: fpn_mobilenet\n",
        "  blocks: 9\n",
        "  d_name: double_gan # may be no_gan, patch_gan, double_gan, multi_scale\n",
        "  d_layers: 3\n",
        "  content_loss: perceptual\n",
        "  adv_lambda: 0.001\n",
        "  disc_loss: wgan-gp\n",
        "  learn_residual: True\n",
        "  norm_layer: instance\n",
        "  dropout: True\n",
        "\n",
        "num_epochs: 200\n",
        "train_batches_per_epoch: 1000\n",
        "val_batches_per_epoch: 100\n",
        "batch_size: 1\n",
        "image_size: [256, 256]\n",
        "\n",
        "optimizer:\n",
        "  name: adam\n",
        "  lr: 0.0001\n",
        "scheduler:\n",
        "  name: linear\n",
        "  start_epoch: 50\n",
        "  min_lr: 0.0000001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOarMZ1Kvjgv",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Changing predict.py\n",
        "%%writefile /content/Colab-DeblurGANv2/predict.py\n",
        "import os\n",
        "from glob import glob\n",
        "from typing import Optional\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import yaml\n",
        "from fire import Fire\n",
        "from tqdm import tqdm\n",
        "\n",
        "from aug import get_normalize\n",
        "from models.networks import get_generator\n",
        "\n",
        "\n",
        "class Predictor:\n",
        "    def __init__(self, weights_path: str, model_name: str = ''):\n",
        "        with open('config/config.yaml') as cfg:\n",
        "            config = yaml.load(cfg)\n",
        "        model = get_generator(model_name or config['model'])\n",
        "        model.load_state_dict(torch.load(weights_path)['model'])\n",
        "        self.model = model.cuda()\n",
        "        self.model.train(True)\n",
        "        # GAN inference should be in train mode to use actual stats in norm layers,\n",
        "        # it's not a bug\n",
        "        self.normalize_fn = get_normalize()\n",
        "\n",
        "    @staticmethod\n",
        "    def _array_to_batch(x):\n",
        "        x = np.transpose(x, (2, 0, 1))\n",
        "        x = np.expand_dims(x, 0)\n",
        "        return torch.from_numpy(x)\n",
        "\n",
        "    def _preprocess(self, x: np.ndarray, mask: Optional[np.ndarray]):\n",
        "        x, _ = self.normalize_fn(x, x)\n",
        "        if mask is None:\n",
        "            mask = np.ones_like(x, dtype=np.float32)\n",
        "        else:\n",
        "            mask = np.round(mask.astype('float32') / 255)\n",
        "\n",
        "        h, w, _ = x.shape\n",
        "        block_size = 32\n",
        "        min_height = (h // block_size + 1) * block_size\n",
        "        min_width = (w // block_size + 1) * block_size\n",
        "\n",
        "        pad_params = {'mode': 'constant',\n",
        "                      'constant_values': 0,\n",
        "                      'pad_width': ((0, min_height - h), (0, min_width - w), (0, 0))\n",
        "                      }\n",
        "        x = np.pad(x, **pad_params)\n",
        "        mask = np.pad(mask, **pad_params)\n",
        "\n",
        "        return map(self._array_to_batch, (x, mask)), h, w\n",
        "\n",
        "    @staticmethod\n",
        "    def _postprocess(x: torch.Tensor) -> np.ndarray:\n",
        "        x, = x\n",
        "        x = x.detach().cpu().float().numpy()\n",
        "        x = (np.transpose(x, (1, 2, 0)) + 1) / 2.0 * 255.0\n",
        "        return x.astype('uint8')\n",
        "\n",
        "    def __call__(self, img: np.ndarray, mask: Optional[np.ndarray], ignore_mask=True) -> np.ndarray:\n",
        "        (img, mask), h, w = self._preprocess(img, mask)\n",
        "        with torch.no_grad():\n",
        "            inputs = [img.cuda()]\n",
        "            if not ignore_mask:\n",
        "                inputs += [mask]\n",
        "            pred = self.model(*inputs)\n",
        "        return self._postprocess(pred)[:h, :w, :]\n",
        "\n",
        "def process_video(pairs, predictor, output_dir):\n",
        "    for video_filepath, mask in tqdm(pairs):\n",
        "        video_filename = os.path.basename(video_filepath)\n",
        "        output_filepath = os.path.join(output_dir, os.path.splitext(video_filename)[0]+'_deblur.mp4')\n",
        "        video_in = cv2.VideoCapture(video_filepath)\n",
        "        fps = video_in.get(cv2.CAP_PROP_FPS)\n",
        "        width = int(video_in.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "        height = int(video_in.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "        total_frame_num = int(video_in.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        video_out = cv2.VideoWriter(output_filepath, cv2.VideoWriter_fourcc(*'MP4V'), fps, (width, height))\n",
        "        tqdm.write(f'process {video_filepath} to {output_filepath}, {fps}fps, resolution: {width}x{height}')\n",
        "        for frame_num in tqdm(range(total_frame_num), desc=video_filename):\n",
        "            res, img = video_in.read()\n",
        "            if not res:\n",
        "                break\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            pred = predictor(img, mask)\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            video_out.write(pred)\n",
        "\n",
        "def main(img_pattern: str,\n",
        "         mask_pattern: Optional[str] = None,\n",
        "         weights_path='/content/Colab-DeblurGANv2/models/fpn_mobilenet.h5',\n",
        "         out_dir='submit/',\n",
        "         side_by_side: bool = False,\n",
        "         video: bool = False):\n",
        "    def sorted_glob(pattern):\n",
        "        return sorted(glob(pattern))\n",
        "\n",
        "    imgs = sorted_glob(img_pattern)\n",
        "    masks = sorted_glob(mask_pattern) if mask_pattern is not None else [None for _ in imgs]\n",
        "    pairs = zip(imgs, masks)\n",
        "    names = sorted([os.path.basename(x) for x in glob(img_pattern)])\n",
        "    predictor = Predictor(weights_path=weights_path)\n",
        "\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    if not video:\n",
        "        for name, pair in tqdm(zip(names, pairs), total=len(names)):\n",
        "            f_img, f_mask = pair\n",
        "            img, mask = map(cv2.imread, (f_img, f_mask))\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "            pred = predictor(img, mask)\n",
        "            if side_by_side:\n",
        "                pred = np.hstack((img, pred))\n",
        "            pred = cv2.cvtColor(pred, cv2.COLOR_RGB2BGR)\n",
        "            cv2.imwrite(os.path.join(out_dir, name),\n",
        "                        pred)\n",
        "    else:\n",
        "        process_video(pairs, predictor, out_dir)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    Fire(main)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w9tNNNpKxiNG",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Changing fpn_mobilenet.py\n",
        "%%writefile /content/Colab-DeblurGANv2/models/fpn_mobilenet.py\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from models.mobilenet_v2 import MobileNetV2\n",
        "\n",
        "class FPNHead(nn.Module):\n",
        "    def __init__(self, num_in, num_mid, num_out):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block0 = nn.Conv2d(num_in, num_mid, kernel_size=3, padding=1, bias=False)\n",
        "        self.block1 = nn.Conv2d(num_mid, num_out, kernel_size=3, padding=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.functional.relu(self.block0(x), inplace=True)\n",
        "        x = nn.functional.relu(self.block1(x), inplace=True)\n",
        "        return x\n",
        "\n",
        "\n",
        "class FPNMobileNet(nn.Module):\n",
        "\n",
        "    def __init__(self, norm_layer, output_ch=3, num_filters=64, num_filters_fpn=128, pretrained=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # Feature Pyramid Network (FPN) with four feature maps of resolutions\n",
        "        # 1/4, 1/8, 1/16, 1/32 and `num_filters` filters for all feature maps.\n",
        "\n",
        "        self.fpn = FPN(num_filters=num_filters_fpn, norm_layer = norm_layer, pretrained=pretrained)\n",
        "\n",
        "        # The segmentation heads on top of the FPN\n",
        "\n",
        "        self.head1 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head2 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head3 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "        self.head4 = FPNHead(num_filters_fpn, num_filters, num_filters)\n",
        "\n",
        "        self.smooth = nn.Sequential(\n",
        "            nn.Conv2d(4 * num_filters, num_filters, kernel_size=3, padding=1),\n",
        "            norm_layer(num_filters),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.smooth2 = nn.Sequential(\n",
        "            nn.Conv2d(num_filters, num_filters // 2, kernel_size=3, padding=1),\n",
        "            norm_layer(num_filters // 2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.final = nn.Conv2d(num_filters // 2, output_ch, kernel_size=3, padding=1)\n",
        "\n",
        "    def unfreeze(self):\n",
        "        self.fpn.unfreeze()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        map0, map1, map2, map3, map4 = self.fpn(x)\n",
        "\n",
        "        map4 = nn.functional.upsample(self.head4(map4), scale_factor=8, mode=\"nearest\")\n",
        "        map3 = nn.functional.upsample(self.head3(map3), scale_factor=4, mode=\"nearest\")\n",
        "        map2 = nn.functional.upsample(self.head2(map2), scale_factor=2, mode=\"nearest\")\n",
        "        map1 = nn.functional.upsample(self.head1(map1), scale_factor=1, mode=\"nearest\")\n",
        "\n",
        "        smoothed = self.smooth(torch.cat([map4, map3, map2, map1], dim=1))\n",
        "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
        "        smoothed = self.smooth2(smoothed + map0)\n",
        "        smoothed = nn.functional.upsample(smoothed, scale_factor=2, mode=\"nearest\")\n",
        "\n",
        "        final = self.final(smoothed)\n",
        "        res = torch.tanh(final) + x\n",
        "\n",
        "        return torch.clamp(res, min=-1, max=1)\n",
        "\n",
        "\n",
        "class FPN(nn.Module):\n",
        "\n",
        "    def __init__(self, norm_layer, num_filters=128, pretrained=True):\n",
        "        \"\"\"Creates an `FPN` instance for feature extraction.\n",
        "        Args:\n",
        "          num_filters: the number of filters in each output pyramid level\n",
        "          pretrained: use ImageNet pre-trained backbone feature extractor\n",
        "        \"\"\"\n",
        "\n",
        "        super().__init__()\n",
        "        net = MobileNetV2(n_class=1000)\n",
        "\n",
        "        if pretrained:\n",
        "            #Load weights into the project directory\n",
        "            state_dict = torch.load('/content/Colab-DeblurGANv2/models/mobilenet_v2.pth.tar') # add map_location='cpu' if no gpu\n",
        "            net.load_state_dict(state_dict)\n",
        "        self.features = net.features\n",
        "\n",
        "        self.enc0 = nn.Sequential(*self.features[0:2])\n",
        "        self.enc1 = nn.Sequential(*self.features[2:4])\n",
        "        self.enc2 = nn.Sequential(*self.features[4:7])\n",
        "        self.enc3 = nn.Sequential(*self.features[7:11])\n",
        "        self.enc4 = nn.Sequential(*self.features[11:16])\n",
        "\n",
        "        self.td1 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "        self.td2 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "        self.td3 = nn.Sequential(nn.Conv2d(num_filters, num_filters, kernel_size=3, padding=1),\n",
        "                                 norm_layer(num_filters),\n",
        "                                 nn.ReLU(inplace=True))\n",
        "\n",
        "        self.lateral4 = nn.Conv2d(160, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral3 = nn.Conv2d(64, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral2 = nn.Conv2d(32, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral1 = nn.Conv2d(24, num_filters, kernel_size=1, bias=False)\n",
        "        self.lateral0 = nn.Conv2d(16, num_filters // 2, kernel_size=1, bias=False)\n",
        "\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def unfreeze(self):\n",
        "        for param in self.features.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Bottom-up pathway, from ResNet\n",
        "        enc0 = self.enc0(x)\n",
        "\n",
        "        enc1 = self.enc1(enc0) # 256\n",
        "\n",
        "        enc2 = self.enc2(enc1) # 512\n",
        "\n",
        "        enc3 = self.enc3(enc2) # 1024\n",
        "\n",
        "        enc4 = self.enc4(enc3) # 2048\n",
        "\n",
        "        # Lateral connections\n",
        "\n",
        "        lateral4 = self.lateral4(enc4)\n",
        "        lateral3 = self.lateral3(enc3)\n",
        "        lateral2 = self.lateral2(enc2)\n",
        "        lateral1 = self.lateral1(enc1)\n",
        "        lateral0 = self.lateral0(enc0)\n",
        "\n",
        "        # Top-down pathway\n",
        "        map4 = lateral4\n",
        "        map3 = self.td1(lateral3 + nn.functional.upsample(map4, scale_factor=2, mode=\"nearest\"))\n",
        "        map2 = self.td2(lateral2 + nn.functional.upsample(map3, scale_factor=2, mode=\"nearest\"))\n",
        "        map1 = self.td3(lateral1 + nn.functional.upsample(map2, scale_factor=2, mode=\"nearest\"))\n",
        "        return lateral0, map1, map2, map3, map4\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fSJm1AEVLu7a",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Downloading model from my Google Drive (Very fast download)\n",
        "!mkdir /root/\n",
        "!mkdir /root/.cache/\n",
        "!mkdir /root/.cache/torch/\n",
        "!mkdir /root/.cache/torch/checkpoints/\n",
        "%cd /root/.cache/torch/checkpoints/\n",
        "!gdown --id 1y6GeaoWjhqjRjrXuZvCYEQYlblZGkE6X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZwavOot5Hbm",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title ```[OPTIONAL / ONLY RUN THIS IF YOU HAVE PROBLEMS WITH THE PREVIOUS CELL]``` Testrun to download model file with pretrainedmodels (Download takes around 10 minutes)\n",
        "%cd /content/\n",
        "!wget --no-check-certificate https://raw.githubusercontent.com/xinntao/ESRGAN/master/LR/baboon.png\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py /content/baboon.png\n",
        "\n",
        "#%cd /content/DeblurGANv2/models\n",
        "#!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU\n",
        "%cd /root/.cache/torch/checkpoints\n",
        "!gdown --id 1JhnT4BBeKBBSLqTo6UsJ13HeBXevarrU\n",
        "%cd /content/Colab-DeblurGANv2/models\n",
        "!wget --no-check-certificate http://sceneparsing.csail.mit.edu/model/pretrained_resnet/mobilenet_v2.pth.tar"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-rlVgQVv4lz",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Run deblur and copy result go Google Drive\n",
        "%cd /content/Colab-DeblurGANv2\n",
        "!python predict.py \"/content/drive/My Drive/Colab-DeblurGANv2/input.png\"\n",
        "!cp /content/Colab-DeblurGANv2/submit/input.png \"/content/drive/My Drive/Colab-DeblurGANv2/output.png\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
